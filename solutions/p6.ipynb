{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DMG2 Assignment : Problem 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Naive Bayes Text Classifier_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of classes : 20\n",
    "\n",
    "In each class, there are a number of documents, each one corresponding to a date. The test-train split will be based on the date. \n",
    "\n",
    "**Preprocessing in each document :**\n",
    "* Keep only From, Subject, Host, Organization, Data\n",
    "* Remove special characters, stop words\n",
    "* Stem the words\n",
    "* There are numbers in the data, as addresses, phone numbers, currency, etc. Should they be removed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk,unicodedata\n",
    "import operator,math\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/jishnu/Documents/ISB/Term3/dmg2/assignments/hw_assignment1/dmg2/datasets/20_newsgroups'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels,files_list = [],[]\n",
    "for root, dirs, files in os.walk(DATA_DIR):\n",
    "    for file in files:\n",
    "        labels.append(re.sub(r'/home/jishnu/Documents/ISB/Term3/dmg2/assignments/hw_assignment1/dmg2/datasets/20_newsgroups/','',root))\n",
    "        files_list.append(os.path.join(root,file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['comp.graphics',\n",
       " 'talk.religion.misc',\n",
       " 'rec.sport.baseball',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'rec.motorcycles',\n",
       " 'talk.politics.guns',\n",
       " 'misc.forsale',\n",
       " 'alt.atheism',\n",
       " 'talk.politics.misc',\n",
       " 'talk.politics.mideast',\n",
       " 'rec.autos',\n",
       " 'comp.windows.x',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'soc.religion.christian',\n",
       " 'rec.sport.hockey',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'sci.space',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'sci.med']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_df = pd.DataFrame({'filename':files_list, 'label' : labels})\n",
    "list(files_df['label'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train - Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each class, splitting the documents to training and test based on a 70-30 rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13997 6000\n"
     ]
    }
   ],
   "source": [
    "train = pd.DataFrame(columns=['filename','label'])\n",
    "test = pd.DataFrame(columns=['filename','label'])\n",
    "\n",
    "for label in list(files_df['label'].unique()):\n",
    "    threshold = files_df.loc[files_df['label'] == label].shape[0] * 0.7\n",
    "    threshold = int(np.floor(threshold))\n",
    "    train = train.append(files_df.loc[files_df['label'] == label].iloc[:threshold,:],ignore_index=True)\n",
    "    test = test.append(files_df.loc[files_df['label'] == label].iloc[threshold:,:],ignore_index=True)\n",
    "\n",
    "print(train.shape[0],test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(files_df.loc[files_df['label'] == 'alt.atheism']['filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(files_list[0],'r') as filein:\n",
    "#    data = filein.read()\n",
    "#    data = re.sub(r'^Xref:.*\\n','\\n',data)\n",
    "#    data = re.sub(r'(^|\\n)(?:Path|Newsgroups|Summary|Message-ID|Expires|Followup-To|Distribution|Approved|Supersedes|Lines):.*\\n','\\n',data)\n",
    "#    data = re.sub(r'(^|\\n)(?:Keywords|Date|Followup-To|Supersedes):.*\\n','\\n',data)\n",
    "#    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer = CountVectorizer(input='filename',analyzer='word',stop_words='english',decode_error='ignore')\n",
    "#vectorizer.fit_transform(list(files_df.loc[files_df['label'] == 'alt.atheism']['filename']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alt_atheism_dtm = pd.DataFrame(vectorizer.fit_transform(list(files_df.loc[files_df['label'] == 'alt.atheism']['filename'])).todense().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alt_atheism_dtm['count_docs'] = alt_atheism_dtm.sum(axis=1)\n",
    "#alt_atheism_dtm['word'] = vectorizer.get_feature_names()\n",
    "#alt_atheism_dtm\n",
    "#alt_atheism_dtm.sort_values(by='count_docs',ascending=False).iloc[:5000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#corpus = [\n",
    "#'All my cats cats in a row',\n",
    "#'Hello, World!']\n",
    "\n",
    "#vectorizer = CountVectorizer()\n",
    "#pd.DataFrame(vectorizer.fit_transform(corpus).todense().T,columns = ['doc1','doc2'])\n",
    "#print( vectorizer.vocabulary_ )\n",
    "#print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating dictionary of 5000 most frequent words in each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating P(W|C) for each word in each class, by normalizing using Laplace smoothing parameter of 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to hold vectorizer objects\n",
    "vect_dict = {}\n",
    "# Dictionary to hold Document term matrix for each class.\n",
    "# The document term matrix is converted to a Pandas DataFrame\n",
    "class_dict = {}\n",
    "for label in list(train['label'].unique()):\n",
    "    vect_dict[label] = CountVectorizer(input='filename',analyzer='word',stop_words='english',decode_error='ignore')\n",
    "    class_dict[label] = pd.DataFrame(vect_dict[label].fit_transform(list(train.loc[train['label'] == label]['filename'])).todense().T)\n",
    "    class_dict[label]['count_docs'] = class_dict[label].sum(axis=1)\n",
    "    class_dict[label]['word'] = vect_dict[label].get_feature_names()\n",
    "    class_dict[label] = class_dict[label].sort_values(by='count_docs',ascending=False).iloc[:5000,:]\n",
    "    tot_freq = class_dict[label]['count_docs'].sum() + 30\n",
    "    class_dict[label]['p(w|c)'] =  class_dict[label]['count_docs'] / (tot_freq + (5000 * 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in list(train['label'].unique()):\n",
    "    class_dict[label] = pd.Series(class_dict[label]['p(w|c)'].values,index=class_dict[label]['word']).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_dict['alt.atheism']['xvxvxvx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_dict['comp.graphics']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words **cmu, edu,com,cs** can be removed for better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Class Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_priors_dict = {}\n",
    "total_freq = 0\n",
    "for label in list(files_df['label'].unique()):\n",
    "    class_priors_dict[label] = files_df.loc[files_df['label'] == label].shape[0]\n",
    "    total_freq += class_priors_dict[label]\n",
    "for label in list(files_df['label'].unique()):\n",
    "    class_priors_dict[label] = np.round(class_priors_dict[label] / total_freq, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comp.graphics': 0.050000000000000003,\n",
       " 'talk.religion.misc': 0.050000000000000003,\n",
       " 'rec.sport.baseball': 0.050000000000000003,\n",
       " 'comp.sys.ibm.pc.hardware': 0.050000000000000003,\n",
       " 'rec.motorcycles': 0.050000000000000003,\n",
       " 'talk.politics.guns': 0.050000000000000003,\n",
       " 'misc.forsale': 0.050000000000000003,\n",
       " 'alt.atheism': 0.050000000000000003,\n",
       " 'talk.politics.misc': 0.050000000000000003,\n",
       " 'talk.politics.mideast': 0.050000000000000003,\n",
       " 'rec.autos': 0.050000000000000003,\n",
       " 'comp.windows.x': 0.050000000000000003,\n",
       " 'sci.crypt': 0.050000000000000003,\n",
       " 'sci.electronics': 0.050000000000000003,\n",
       " 'soc.religion.christian': 0.0499,\n",
       " 'rec.sport.hockey': 0.050000000000000003,\n",
       " 'comp.os.ms-windows.misc': 0.050000000000000003,\n",
       " 'sci.space': 0.050000000000000003,\n",
       " 'comp.sys.mac.hardware': 0.050000000000000003,\n",
       " 'sci.med': 0.050000000000000003}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_priors_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words = []\n",
    "for train_doc in train['filename']:\n",
    "    with open(train_doc,'r',encoding='ISO-8859-1') as filein:\n",
    "        words = nltk.word_tokenize(filein.read())\n",
    "        words = normalize(words)\n",
    "        train_words.append(words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predicted = pd.DataFrame(columns=['predicted','max_class_posterior_prob'])\n",
    "for train_words_list in train_words:\n",
    "    log_posterior_dict = class_priors_dict\n",
    "    log_posterior_dict = dict([(k,math.log(v)) for (k,v) in log_posterior_dict.items()])\n",
    "    for word in train_words_list:\n",
    "        for k,v in log_posterior_dict.items():\n",
    "            try:\n",
    "                log_posterior_dict[k] = log_posterior_dict[k] + math.log(class_dict[k][word])\n",
    "            except:\n",
    "                pass\n",
    "    log_posterior_dict = dict([(k,np.exp(v)) for (k,v) in log_posterior_dict.items()])\n",
    "    train_predicted = train_predicted.append({'predicted':max(log_posterior_dict, key=log_posterior_dict.get),'max_class_posterior_prob':max(log_posterior_dict.values())},ignore_index=True)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>max_class_posterior_prob</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>1.110726e-130</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rec.sport.baseball</td>\n",
       "      <td>1.571055e-113</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>5.849009e-158</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rec.sport.baseball</td>\n",
       "      <td>5.692217e-113</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rec.sport.baseball</td>\n",
       "      <td>7.939355e-142</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               predicted  max_class_posterior_prob         actual\n",
       "0  talk.politics.mideast             1.110726e-130  comp.graphics\n",
       "1     rec.sport.baseball             1.571055e-113  comp.graphics\n",
       "2  talk.politics.mideast             5.849009e-158  comp.graphics\n",
       "3     rec.sport.baseball             5.692217e-113  comp.graphics\n",
       "4     rec.sport.baseball             7.939355e-142  comp.graphics"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predicted['actual'] = train['label']\n",
    "train_predicted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81, 3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predicted.loc[train_predicted['predicted'] == train_predicted['actual']].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13997, 3)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005786954347360149"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "81/13997"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
