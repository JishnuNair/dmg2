{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DMG2 Assignment : Problem 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Naive Bayes Text Classifier_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of classes : 20\n",
    "\n",
    "In each class, there are a number of documents, each one corresponding to a date. The test-train split will be based on the date. \n",
    "\n",
    "**Preprocessing in each document :**\n",
    "* Keep only From, Subject, Host, Organization, Data\n",
    "* Remove special characters, stop words\n",
    "* Stem the words\n",
    "* There are numbers in the data, as addresses, phone numbers, currency, etc. Should they be removed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk,unicodedata\n",
    "import operator,math\n",
    "import inflect\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/jishnu/Documents/ISB/Term3/dmg2/assignments/hw_assignment1/dmg2/datasets/20_newsgroups'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels,files_list = [],[]\n",
    "for root, dirs, files in os.walk(DATA_DIR):\n",
    "    for file in files:\n",
    "        labels.append(re.sub(r'/home/jishnu/Documents/ISB/Term3/dmg2/assignments/hw_assignment1/dmg2/datasets/20_newsgroups/','',root))\n",
    "        files_list.append(os.path.join(root,file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['comp.graphics',\n",
       " 'talk.religion.misc',\n",
       " 'rec.sport.baseball',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'rec.motorcycles',\n",
       " 'talk.politics.guns',\n",
       " 'misc.forsale',\n",
       " 'alt.atheism',\n",
       " 'talk.politics.misc',\n",
       " 'talk.politics.mideast',\n",
       " 'rec.autos',\n",
       " 'comp.windows.x',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'soc.religion.christian',\n",
       " 'rec.sport.hockey',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'sci.space',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'sci.med']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_df = pd.DataFrame({'filename':files_list, 'label' : labels})\n",
    "list(files_df['label'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train - Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each class, splitting the documents to training and test based on a 70-30 rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13997 6000\n"
     ]
    }
   ],
   "source": [
    "train = pd.DataFrame(columns=['filename','label'])\n",
    "test = pd.DataFrame(columns=['filename','label'])\n",
    "\n",
    "for label in list(files_df['label'].unique()):\n",
    "    threshold = files_df.loc[files_df['label'] == label].shape[0] * 0.7\n",
    "    threshold = int(np.floor(threshold))\n",
    "    train = train.append(files_df.loc[files_df['label'] == label].iloc[:threshold,:],ignore_index=True)\n",
    "    test = test.append(files_df.loc[files_df['label'] == label].iloc[threshold:,:],ignore_index=True)\n",
    "\n",
    "print(train.shape[0],test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating dictionary of 5000 most frequent words in each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating P(W|C) for each word in each class, by normalizing using Laplace smoothing parameter of 30.\n",
    "\n",
    "Here, CountVectorizer class from scikit-learn has been used to create the Document-Term Matrix. The class has an inbuilt preprocessing module.\n",
    "After calculating the document term matrix, the counts of document in which each word occurs has been calculated to find the most frequent ones for each class. Then, the probability of word given class has been calculated for the top 5000 words. Laplace smoothing parameter of 30 has been used when calculating P(W|C)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            pass\n",
    "            #new_word = p.number_to_words(word)\n",
    "            #new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = replace_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to hold vectorizer objects\n",
    "vect_dict = {}\n",
    "# Dictionary to hold Document term matrix for each class.\n",
    "# The document term matrix is converted to a Pandas DataFrame\n",
    "class_dict = {}\n",
    "for label in list(train['label'].unique()):\n",
    "    # List to hold words for each label\n",
    "    class_words_preprocessed = []\n",
    "    for filename in train.loc[train['label'] == label]['filename']:\n",
    "        with open(filename,'r',errors='ignore') as filein:\n",
    "            data = filein.read()\n",
    "        words = re.split('\\W+',data)\n",
    "        words = normalize(words)\n",
    "        class_words_preprocessed.append(' '.join(words))\n",
    "    vect_dict[label] = CountVectorizer(input='content',analyzer='word',decode_error='ignore')\n",
    "    class_dict[label] = pd.DataFrame(vect_dict[label].fit_transform(class_words_preprocessed).todense().T)\n",
    "    class_dict[label]['count_docs'] = class_dict[label].sum(axis=1)\n",
    "    class_dict[label]['word'] = vect_dict[label].get_feature_names()\n",
    "    class_dict[label] = class_dict[label].sort_values(by='count_docs',ascending=False).iloc[:5000,:]\n",
    "    tot_freq = class_dict[label]['count_docs'].sum()\n",
    "    class_dict[label]['p(w|c)'] =  (class_dict[label]['count_docs'] + 30) / (tot_freq + (5000 * 30))        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dictionary to hold vectorizer objects\n",
    "# vect_dict = {}\n",
    "# # Dictionary to hold Document term matrix for each class.\n",
    "# # The document term matrix is converted to a Pandas DataFrame\n",
    "# class_dict = {}\n",
    "# for label in list(train['label'].unique()):\n",
    "#     vect_dict[label] = CountVectorizer(input='filename',analyzer='word',stop_words='english',decode_error='ignore')\n",
    "#     class_dict[label] = pd.DataFrame(vect_dict[label].fit_transform(list(train.loc[train['label'] == label]['filename'])).todense().T)\n",
    "#     class_dict[label]['count_docs'] = class_dict[label].sum(axis=1)\n",
    "#     class_dict[label]['word'] = vect_dict[label].get_feature_names()\n",
    "#     class_dict[label] = class_dict[label].sort_values(by='count_docs',ascending=False).iloc[:5000,:]\n",
    "#     tot_freq = class_dict[label]['count_docs'].sum()\n",
    "#     class_dict[label]['p(w|c)'] =  (class_dict[label]['count_docs'] + 30) / (tot_freq + (5000 * 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the top 25 most frequent words for all labels, are there any words which occur in all the documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'newsgroups', 'organization', 'path', 'date', 'the', 'lines', 'messageid', 'subject', 'from'}\n"
     ]
    }
   ],
   "source": [
    "top25_list = []\n",
    "for label in list(train['label'].unique()):\n",
    "    top25_list.append(class_dict[label].iloc[:25,:]['word'])\n",
    "intersect = set(top25_list[0])\n",
    "for list_ in top25_list[1:]:\n",
    "    intersect.intersection_update(list_)\n",
    "print(intersect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing these words from each dictionary and recalculating probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jishnu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "for label in list(train['label'].unique()):\n",
    "    class_dict[label] = class_dict[label].loc[~ class_dict[label]['word'].isin(list(intersect))]\n",
    "for label in list(train['label'].unique()):\n",
    "    tot_freq = class_dict[label]['count_docs'].sum()\n",
    "    class_dict[label]['p(w|c)'] =  (class_dict[label]['count_docs'] + 30) / (tot_freq + (5000 * 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Word Dictionary for each class\n",
    "for label in list(train['label'].unique()):\n",
    "    class_dict[label] = pd.Series(class_dict[label]['p(w|c)'].values,index=class_dict[label]['word']).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_dict['comp.graphics']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words **cmu, edu,com,cs** can be removed for better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Class Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_priors_dict = {}\n",
    "total_freq = 0\n",
    "for label in list(files_df['label'].unique()):\n",
    "    class_priors_dict[label] = files_df.loc[files_df['label'] == label].shape[0]\n",
    "    total_freq += class_priors_dict[label]\n",
    "for label in list(files_df['label'].unique()):\n",
    "    class_priors_dict[label] = np.round(class_priors_dict[label] / total_freq, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comp.graphics': 0.050000000000000003,\n",
       " 'talk.religion.misc': 0.050000000000000003,\n",
       " 'rec.sport.baseball': 0.050000000000000003,\n",
       " 'comp.sys.ibm.pc.hardware': 0.050000000000000003,\n",
       " 'rec.motorcycles': 0.050000000000000003,\n",
       " 'talk.politics.guns': 0.050000000000000003,\n",
       " 'misc.forsale': 0.050000000000000003,\n",
       " 'alt.atheism': 0.050000000000000003,\n",
       " 'talk.politics.misc': 0.050000000000000003,\n",
       " 'talk.politics.mideast': 0.050000000000000003,\n",
       " 'rec.autos': 0.050000000000000003,\n",
       " 'comp.windows.x': 0.050000000000000003,\n",
       " 'sci.crypt': 0.050000000000000003,\n",
       " 'sci.electronics': 0.050000000000000003,\n",
       " 'soc.religion.christian': 0.0499,\n",
       " 'rec.sport.hockey': 0.050000000000000003,\n",
       " 'comp.os.ms-windows.misc': 0.050000000000000003,\n",
       " 'sci.space': 0.050000000000000003,\n",
       " 'comp.sys.mac.hardware': 0.050000000000000003,\n",
       " 'sci.med': 0.050000000000000003}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_priors_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy : 0.05\n"
     ]
    }
   ],
   "source": [
    "train_predicted = pd.DataFrame(columns=['predicted','max_class_posterior_prob'])\n",
    "for train_doc in list(train['filename']):\n",
    "    with open(train_doc,'r',errors='ignore') as filein:\n",
    "            data = filein.read()\n",
    "    words = re.split('\\W+',data)\n",
    "    words = normalize(words)\n",
    "    log_posterior_dict = class_priors_dict\n",
    "    log_posterior_dict = dict([(k,math.log(v)) for (k,v) in log_posterior_dict.items()])\n",
    "    for word in words:\n",
    "        for k,v in log_posterior_dict.items():\n",
    "            try:\n",
    "                log_posterior_dict[k] = log_posterior_dict[k] + math.log(class_dict[k][word])\n",
    "            except:\n",
    "                pass\n",
    "    log_posterior_dict = dict([(k,np.exp(v)) for (k,v) in log_posterior_dict.items()])\n",
    "    train_predicted = train_predicted.append({'predicted':max(log_posterior_dict, key=log_posterior_dict.get),'max_class_posterior_prob':max(log_posterior_dict.values())},ignore_index=True)\n",
    "train_predicted['actual'] = train['label']   \n",
    "print('Training Accuracy : {}'.format(np.round(train_predicted.loc[train_predicted['predicted'] == train_predicted['actual']].shape[0]/train_predicted.shape[0],4)))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_predicted = pd.DataFrame(columns=['predicted','max_class_posterior_prob'])\n",
    "# for train_doc in list(train['filename']):\n",
    "#     vect_train = CountVectorizer(input='filename',analyzer='word',stop_words='english',decode_error='ignore')\n",
    "#     train_docterm = pd.DataFrame(vect_train.fit_transform([train_doc]).todense().T)\n",
    "#     #print(vect_train.get_feature_names())\n",
    "#     log_posterior_dict = class_priors_dict\n",
    "#     log_posterior_dict = dict([(k,math.log(v)) for (k,v) in log_posterior_dict.items()])\n",
    "#     for word in vect_train.get_feature_names():\n",
    "#         for k,v in log_posterior_dict.items():\n",
    "#             try:\n",
    "#                 log_posterior_dict[k] = log_posterior_dict[k] + math.log(class_dict[k][word])\n",
    "#             except:\n",
    "#                 pass\n",
    "#     log_posterior_dict = dict([(k,np.exp(v)) for (k,v) in log_posterior_dict.items()])\n",
    "#     train_predicted = train_predicted.append({'predicted':max(log_posterior_dict, key=log_posterior_dict.get),'max_class_posterior_prob':max(log_posterior_dict.values())},ignore_index=True)\n",
    "# train_predicted['actual'] = train['label']   \n",
    "# print('Training Accuracy : {}'.format(np.round(train_predicted.loc[train_predicted['predicted'] == train_predicted['actual']].shape[0]/train_predicted.shape[0],4)))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy : 0.05\n"
     ]
    }
   ],
   "source": [
    "test_predicted = pd.DataFrame(columns=['predicted','max_class_posterior_prob'])\n",
    "for test_doc in list(test['filename']):\n",
    "    with open(test_doc,'r',errors='ignore') as filein:\n",
    "            data = filein.read()\n",
    "    words = re.split('\\W+',data)\n",
    "    words = normalize(words)\n",
    "    log_posterior_dict = class_priors_dict\n",
    "    log_posterior_dict = dict([(k,math.log(v)) for (k,v) in log_posterior_dict.items()])\n",
    "    for word in words:\n",
    "        for k,v in log_posterior_dict.items():\n",
    "            try:\n",
    "                log_posterior_dict[k] = log_posterior_dict[k] + math.log(class_dict[k][word])\n",
    "            except:\n",
    "                pass\n",
    "    log_posterior_dict = dict([(k,np.exp(v)) for (k,v) in log_posterior_dict.items()])\n",
    "    test_predicted = test_predicted.append({'predicted':max(log_posterior_dict, key=log_posterior_dict.get),'max_class_posterior_prob':max(log_posterior_dict.values())},ignore_index=True)\n",
    "test_predicted['actual'] = test['label']   \n",
    "print('Testing Accuracy : {}'.format(np.round(test_predicted.loc[test_predicted['predicted'] == test_predicted['actual']].shape[0]/test_predicted.shape[0],4)))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is seen that the training and test accuracy are 5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating dictionary of 10,000 most frequent words in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to hold vectorizer objects\n",
    "vect_dict = {}\n",
    "# Dictionary to hold Document term matrix for each class.\n",
    "# The document term matrix is converted to a Pandas DataFrame\n",
    "class_dict = {}\n",
    "for label in list(train['label'].unique()):\n",
    "    # List to hold words for each label\n",
    "    class_words_preprocessed = []\n",
    "    for filename in train.loc[train['label'] == label]['filename']:\n",
    "        with open(filename,'r',errors='ignore') as filein:\n",
    "            data = filein.read()\n",
    "        words = re.split('\\W+',data)\n",
    "        words = normalize(words)\n",
    "        class_words_preprocessed.append(' '.join(words))\n",
    "    vect_dict[label] = CountVectorizer(input='content',analyzer='word',decode_error='ignore')\n",
    "    class_dict[label] = pd.DataFrame(vect_dict[label].fit_transform(class_words_preprocessed).todense().T)\n",
    "    class_dict[label]['count_docs'] = class_dict[label].sum(axis=1)\n",
    "    class_dict[label]['word'] = vect_dict[label].get_feature_names()\n",
    "    class_dict[label] = class_dict[label].sort_values(by='count_docs',ascending=False).iloc[:10000,:]\n",
    "    tot_freq = class_dict[label]['count_docs'].sum()\n",
    "    class_dict[label]['p(w|c)'] =  (class_dict[label]['count_docs'] + 30) / (tot_freq + (10000 * 30))        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dictionary to hold vectorizer objects\n",
    "# vect_dict = {}\n",
    "# # Dictionary to hold Document term matrix for each class.\n",
    "# # The document term matrix is converted to a Pandas DataFrame\n",
    "# class_dict = {}\n",
    "# for label in list(train['label'].unique()):\n",
    "#     vect_dict[label] = CountVectorizer(input='filename',analyzer='word',stop_words='english',decode_error='ignore')\n",
    "#     class_dict[label] = pd.DataFrame(vect_dict[label].fit_transform(list(train.loc[train['label'] == label]['filename'])).todense().T)\n",
    "#     class_dict[label]['count_docs'] = class_dict[label].sum(axis=1)\n",
    "#     class_dict[label]['word'] = vect_dict[label].get_feature_names()\n",
    "#     class_dict[label] = class_dict[label].sort_values(by='count_docs',ascending=False).iloc[:10000,:]\n",
    "#     tot_freq = class_dict[label]['count_docs'].sum()\n",
    "#     class_dict[label]['p(w|c)'] =  (class_dict[label]['count_docs'] + 30) / (tot_freq + (10000 * 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'edu', 'cmu', 'cs', 'net', 'cantaloupe', 'subject', 'srv', 'message', 'com'}\n"
     ]
    }
   ],
   "source": [
    "top25_list = []\n",
    "for label in list(train['label'].unique()):\n",
    "    top25_list.append(class_dict[label].iloc[:25,:]['word'])\n",
    "intersect = set(top25_list[0])\n",
    "for list_ in top25_list[1:]:\n",
    "    intersect.intersection_update(list_)\n",
    "print(intersect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing these words from each dictionary and recalculating probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in list(train['label'].unique()):\n",
    "    class_dict[label] = class_dict[label].loc[~ class_dict[label]['word'].isin(list(intersect))]\n",
    "for label in list(train['label'].unique()):\n",
    "    tot_freq = class_dict[label]['count_docs'].sum()\n",
    "    class_dict[label]['p(w|c)'] =  (class_dict[label]['count_docs'] + 30) / (tot_freq + (5000 * 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Word Dictionary for each class\n",
    "for label in list(train['label'].unique()):\n",
    "    class_dict[label] = pd.Series(class_dict[label]['p(w|c)'].values,index=class_dict[label]['word']).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy : 0.0162\n"
     ]
    }
   ],
   "source": [
    "train_predicted = pd.DataFrame(columns=['predicted','max_class_posterior_prob'])\n",
    "for train_doc in list(train['filename']):\n",
    "    with open(train_doc,'r',errors='ignore') as filein:\n",
    "            data = filein.read()\n",
    "    words = re.split('\\W+',data)\n",
    "    words = normalize(words)\n",
    "    log_posterior_dict = class_priors_dict\n",
    "    log_posterior_dict = dict([(k,math.log(v)) for (k,v) in log_posterior_dict.items()])\n",
    "    for word in words:\n",
    "        for k,v in log_posterior_dict.items():\n",
    "            try:\n",
    "                log_posterior_dict[k] = log_posterior_dict[k] + math.log(class_dict[k][word])\n",
    "            except:\n",
    "                pass\n",
    "    log_posterior_dict = dict([(k,np.exp(v)) for (k,v) in log_posterior_dict.items()])\n",
    "    train_predicted = train_predicted.append({'predicted':max(log_posterior_dict, key=log_posterior_dict.get),'max_class_posterior_prob':max(log_posterior_dict.values())},ignore_index=True)\n",
    "train_predicted['actual'] = train['label']   \n",
    "print('Training Accuracy : {}'.format(np.round(train_predicted.loc[train_predicted['predicted'] == train_predicted['actual']].shape[0]/train_predicted.shape[0],4)))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy : 0.0175\n"
     ]
    }
   ],
   "source": [
    "test_predicted = pd.DataFrame(columns=['predicted','max_class_posterior_prob'])\n",
    "for test_doc in list(test['filename']):\n",
    "    with open(test_doc,'r',errors='ignore') as filein:\n",
    "            data = filein.read()\n",
    "    words = re.split('\\W+',data)\n",
    "    words = normalize(words)\n",
    "    log_posterior_dict = class_priors_dict\n",
    "    log_posterior_dict = dict([(k,math.log(v)) for (k,v) in log_posterior_dict.items()])\n",
    "    for word in words:\n",
    "        for k,v in log_posterior_dict.items():\n",
    "            try:\n",
    "                log_posterior_dict[k] = log_posterior_dict[k] + math.log(class_dict[k][word])\n",
    "            except:\n",
    "                pass\n",
    "    log_posterior_dict = dict([(k,np.exp(v)) for (k,v) in log_posterior_dict.items()])\n",
    "    test_predicted = test_predicted.append({'predicted':max(log_posterior_dict, key=log_posterior_dict.get),'max_class_posterior_prob':max(log_posterior_dict.values())},ignore_index=True)\n",
    "test_predicted['actual'] = test['label']   \n",
    "print('Testing Accuracy : {}'.format(np.round(test_predicted.loc[test_predicted['predicted'] == test_predicted['actual']].shape[0]/test_predicted.shape[0],4)))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It is seen that increasing the dictionary to 10,000 increases the training and test accuracies to 2.9% and 3.6%. This can be improved further.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
